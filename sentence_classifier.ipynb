{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-based Text Analytics of CyberSecurity Strategies\n",
    "Uses machine learning to calssify sentences from CyberSecurity documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These labels come from the headers in the cyberwellness profiles linked above**\n",
    "\n",
    "| Category               | Sub category |\n",
    "|------------------------| -------------|\n",
    "|LEGAL MEASURES          | CRIMINAL LEGISLATION, REGULATION AND COMPLIANCE|\n",
    "|TECHNICAL MEASURES      | CIRT, STANDARDS, CERTIFICATION|\n",
    "|ORGANIZATION MEASURES   | POLICY, ROADMAP FOR GOVERNANCE, RESPONSIBLE AGENCY, NATIONAL BENCHMARKING|\n",
    "|CAPACITY BUILDING       | STANDARDISATION DEVELOPMENT, MANPOWER DEVELOPMENT, PROFESSIONAL CERTIFICATION, AGENCY CERTIFICATION|\n",
    "|COOPERATION             | INTRA-STATE COOPERATION, INTRA-AGENCY COOPERATION, PUBLIC SECTOR PARTNERSHIP,  INTERNATIONAL COOPERATION|\n",
    "|CHILD ONLINE PROTECTION | NATIONAL LEGISLATION,  UN CONVENTION AND PROTOCOL, INSTITUTIONAL SUPPORT, REPORTING MECHANISM|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://nltk.github.com/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When using nltk for the first time, uncomment the following lines and run cell.\n",
    "# nltk.download must only be downloaded once\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 3\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "The following cells read in training samples from a json file, create a lexicon from it, create arrays that store the number of occurences of each word in the lexicon, and serialize the generated list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Country': u'Jordan',\n",
      "  u'sentence': u'However, these approaches: are generally basic; not systematic; subjective; have no clear definition or boundaries, are not thorough; do not meet international standards; and do not deal effectively with threats emerging from cyberspace.',\n",
      "  u'sentence_id,': u'ff30d97ab4',\n",
      "  u'tag': [{u'category': u'technical measures',\n",
      "            u'subcategory': [u'standards']}]},\n",
      " {u'Country': u'Jordan',\n",
      "  u'sentence': u'Strategies and policies developed by the private sector should augment, comply, and be consistent with this strategy.',\n",
      "  u'sentence_id,': u'e50e3676b6',\n",
      "  u'tag': [{u'category': u'organization measures',\n",
      "            u'subcategory': [u'policy']}]},\n",
      " {u'Country': u'Jordan',\n",
      "  u'sentence': u'security policy and role-based security responsibilities will have a higher rate of success in protecting critical information.',\n",
      "  u'sentence_id,': u'ddd832b614',\n",
      "  u'tag': [{u'category': u'organization measures',\n",
      "            u'subcategory': [u'policy']}]}]\n"
     ]
    }
   ],
   "source": [
    "# Opens training data stored as Json and converts to Python list\n",
    "with open('results_concatenated.json') as f:    \n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples is 2045 \n",
      "\n",
      "First example is \n",
      "X: However, these approaches: are generally basic; not systematic; subjective; have no clear definition or boundaries, are not thorough; do not meet international standards; and do not deal effectively with threats emerging from cyberspace. \n",
      "\n",
      " y: [{u'category': u'technical measures', u'subcategory': [u'standards']}]\n"
     ]
    }
   ],
   "source": [
    "# Splits data into 3 parts, IDs, sentences, and tags\n",
    "\n",
    "# For testing purposes\n",
    "sentence_ids = []\n",
    "\n",
    "# Lexicons created from sentences will be inputs\n",
    "sentences  = []\n",
    "\n",
    "# Tags will be outputs\n",
    "tags = []\n",
    "\n",
    "for input_val in data:\n",
    "    sentence_ids.append(input_val[u'sentence_id,'])\n",
    "    sentences.append(input_val[u'sentence'])\n",
    "    tags.append(input_val[u'tag'])\n",
    "\n",
    "print(\"Number of training examples is {} \\n\".format(len(sentences)))\n",
    "print(\"First example is \\nX: {} \\n\\n y: {}\".format(sentences[0], tags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates lexicon (list of unique words) from all training samples\n",
    "def create_lexicon(sentences):\n",
    "    lexicon = []\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence):\n",
    "            root = lemmatizer.lemmatize(word.lower()).encode('utf-8')\n",
    "            if len(root) > 1 and root not in stop and root not in lexicon:\n",
    "                lexicon.append(root)\n",
    "    \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates 2D array containing number of occurences of each word in lexicon in each sample\n",
    "def produce_X(sentences, lexicon):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        X_sample = [0 for _ in lexicon]\n",
    "        for word in word_tokenize(sentence):\n",
    "            root = lemmatizer.lemmatize(word.lower()).encode('utf-8')\n",
    "            if root in lexicon:\n",
    "                X_sample[lexicon.index(root)] += 1\n",
    "        \n",
    "        X.append(X_sample)\n",
    "    \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'approach', 'generally', 'basic', 'systematic', 'subjective']\n"
     ]
    }
   ],
   "source": [
    "sample_lexicon = create_lexicon(sentences)\n",
    "pprint(sample_lexicon[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = produce_X(sentences, sample_lexicon)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickles features generated for reuse\n",
    "\n",
    "with open('sample_X.npy', 'wb') as f:\n",
    "    np.save(f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories (index 0-5)\n",
    "0. LEGAL MEASURES\n",
    "1. TECHNICAL MEASURES\n",
    "2. ORGANIZATION MEASURES\n",
    "3. CAPACITY BUILDING\n",
    "4. COOPERATION\n",
    "5. CHILD ONLINE PROTECTION\n",
    "\n",
    "> Categories will be stored as a 1D array with each number corresponding to a category listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary stores label names and corresponding index to be turned on in the one hot vector.\n",
    "category_dict = {\n",
    "    u'LEGAL MEASURES' : 0,\n",
    "    u'TECHNICAL MEASURES' : 1,\n",
    "    u'ORGANIZATION MEASURES' : 2,\n",
    "    u'CAPACITY BUILDING' : 3,\n",
    "    u'COOPERATION' : 4,\n",
    "    u'CHILD ONLINE PROTECTION' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_y(tags):\n",
    "    return np.array([category_dict[tag[0][u'category'].upper()] for tag in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "y = produce_y(tags)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tags.npy', 'wb') as f:\n",
    "    np.save(f, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorflow Boilerplate\n",
    "To simplify the Tensorflow code, we will define a set of functions to delare variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(X, W, b, name='fc'):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.relu(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Now that the data has been processed it is now time to load the data and fit a model to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_X.npy\",\"rb\") as f:\n",
    "    X = np.load(f)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "with open(\"tags.npy\",\"rb\") as f:\n",
    "    y = np.load(f)\n",
    "\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_options = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(vector):\n",
    "    def hot_or_not(i, j):\n",
    "        return 1 if i == j else 0\n",
    "    return np.array([[int(hot_or_not(i, j)) for j in range(number_of_options)] for i in vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size=100):\n",
    "    n_batches = len(X) / batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = (batch * batch_size) % len(X)\n",
    "        end = start + batch_size if start + batch_size < len(X) else len(X) - 1\n",
    "        yield X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the Model\n",
    "Now we can create a neural network to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_size = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_placeholder = tf.placeholder(tf.float32, [None, lexicon_size])\n",
    "y_placeholder = tf.placeholder(tf.float32, [None, number_of_options])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = weight_variable([lexicon_size, 10])\n",
    "b1 = bias_variable([10])\n",
    "\n",
    "model = fc_layer(X_placeholder, w1, b1, name='fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2 = weight_variable([10, 10])\n",
    "b2 = bias_variable([10])\n",
    "\n",
    "model = fc_layer(model, w2, b2, name='fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w3 = weight_variable([10, number_of_options])\n",
    "b3 = bias_variable([number_of_options])\n",
    "\n",
    "y_predicted = tf.matmul(model, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=y_placeholder, logits=y_predicted))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splitting up the Data\n",
    "Now we have to split up the data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " ..., \n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "1840\n",
      "1840\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.9\n",
    "y_hot = one_hot(y)\n",
    "print(y_hot)\n",
    "\n",
    "X_split_index = int(len(X)*TEST_SIZE)\n",
    "y_split_index = int(len(y_hot)*TEST_SIZE)\n",
    "\n",
    "X_train, X_test = X[:X_split_index], X[X_split_index:]\n",
    "y_train, y_test = y_hot[:y_split_index], y_hot[y_split_index:]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session starting\n",
      "batch number: 0\n",
      "Train accuracy 0.429891318083\n",
      "batch number: 1\n",
      "Train accuracy 0.483695656061\n",
      "batch number: 2\n",
      "Train accuracy 0.483695656061\n",
      "batch number: 3\n",
      "Train accuracy 0.483695656061\n",
      "batch number: 4\n",
      "Train accuracy 0.541304349899\n",
      "batch number: 5\n",
      "Train accuracy 0.526630461216\n",
      "batch number: 6\n",
      "Train accuracy 0.4375\n",
      "batch number: 7\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session() as sess:\n",
    "        print(\"Session starting\")\n",
    "        \n",
    "        def accuracy(Xt, yt):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_predicted,1), tf.argmax(y_placeholder,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            return sess.run(accuracy, feed_dict={X_placeholder: Xt, y_placeholder: yt})\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(100):\n",
    "            epoch_loss = 0\n",
    "            avg_cost = 0.0\n",
    "            for i, (batch_X, batch_y) in enumerate(next_batch(X_train, y_train)):\n",
    "                print('batch number: {}'.format(i))\n",
    "                # print(\"\\nBatch {}\\n\".format(batch_y))\n",
    "                sess.run(optimizer, feed_dict={X_placeholder: batch_X, y_placeholder: batch_y})\n",
    "                print(\"Train accuracy {}\".format(accuracy(X_train, y_train)))\n",
    "\n",
    "        print(\"Final Train accuracy {}\".format(accuracy(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
