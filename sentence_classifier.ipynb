{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-based Text Analytics of CyberSecurity Strategies\n",
    "Uses machine learning to calssify sentences from CyberSecurity documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These labels come from the headers in the cyberwellness profiles linked above**\n",
    "\n",
    "| Category               | Sub category |\n",
    "|------------------------| -------------|\n",
    "|LEGAL MEASURES          | CRIMINAL LEGISLATION, REGULATION AND COMPLIANCE|\n",
    "|TECHNICAL MEASURES      | CIRT, STANDARDS, CERTIFICATION|\n",
    "|ORGANIZATION MEASURES   | POLICY, ROADMAP FOR GOVERNANCE, RESPONSIBLE AGENCY, NATIONAL BENCHMARKING|\n",
    "|CAPACITY BUILDING       | STANDARDISATION DEVELOPMENT, MANPOWER DEVELOPMENT, PROFESSIONAL CERTIFICATION, AGENCY CERTIFICATION|\n",
    "|COOPERATION             | INTRA-STATE COOPERATION, INTRA-AGENCY COOPERATION, PUBLIC SECTOR PARTNERSHIP,  INTERNATIONAL COOPERATION|\n",
    "|CHILD ONLINE PROTECTION | NATIONAL LEGISLATION,  UN CONVENTION AND PROTOCOL, INSTITUTIONAL SUPPORT, REPORTING MECHANISM|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When using nltk for the first time, uncomment the following lines and run cell.\n",
    "# nltk.download must only be downloaded once\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "The following cells read in training samples from a json file, create a lexicon from it, create arrays that store the number of occurences of each word in the lexicon, and serialize the generated list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Country': 'Jordan',\n",
      "  'sentence': 'However, these approaches: are generally basic; not systematic; '\n",
      "              'subjective; have no clear definition or boundaries, are not '\n",
      "              'thorough; do not meet international standards; and do not deal '\n",
      "              'effectively with threats emerging from cyberspace.',\n",
      "  'sentence_id,': 'ff30d97ab4',\n",
      "  'tag': [{'category': 'technical measures', 'subcategory': ['standards']}]},\n",
      " {'Country': 'Jordan',\n",
      "  'sentence': 'Strategies and policies developed by the private sector should '\n",
      "              'augment, comply, and be consistent with this strategy.',\n",
      "  'sentence_id,': 'e50e3676b6',\n",
      "  'tag': [{'category': 'organization measures', 'subcategory': ['policy']}]},\n",
      " {'Country': 'Jordan',\n",
      "  'sentence': 'security policy and role-based security responsibilities will '\n",
      "              'have a higher rate of success in protecting critical '\n",
      "              'information.',\n",
      "  'sentence_id,': 'ddd832b614',\n",
      "  'tag': [{'category': 'organization measures', 'subcategory': ['policy']}]}]\n"
     ]
    }
   ],
   "source": [
    "# Opens training data stored as Json and converts to Python list\n",
    "with open('results_concatenated.json') as f:    \n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples is 2045 \n",
      "\n",
      "First example is \n",
      "X: However, these approaches: are generally basic; not systematic; subjective; have no clear definition or boundaries, are not thorough; do not meet international standards; and do not deal effectively with threats emerging from cyberspace. \n",
      "\n",
      " y: [{'category': 'technical measures', 'subcategory': ['standards']}]\n"
     ]
    }
   ],
   "source": [
    "# Splits data into 3 parts, IDs, sentences, and tags\n",
    "\n",
    "# For testing purposes\n",
    "sentence_ids = []\n",
    "\n",
    "# Lexicons created from sentences will be inputs\n",
    "sentences  = []\n",
    "\n",
    "# Tags will be outputs\n",
    "tags = []\n",
    "\n",
    "for input_val in data:\n",
    "    sentence_ids.append(input_val[u'sentence_id,'])\n",
    "    sentences.append(input_val[u'sentence'])\n",
    "    tags.append(input_val[u'tag'])\n",
    "\n",
    "print(\"Number of training examples is {} \\n\".format(len(sentences)))\n",
    "print(\"First example is \\nX: {} \\n\\n y: {}\".format(sentences[0], tags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates lexicon (list of unique words) from all training samples\n",
    "def create_lexicon(sentences):\n",
    "    lexicon = []\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence):\n",
    "            root = lemmatizer.lemmatize(word.lower()).encode('utf-8')\n",
    "            if len(root) > 1 and root not in stop and root not in lexicon:\n",
    "                lexicon.append(root)\n",
    "    \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates 2D array containing number of occurences of each word in lexicon in each sample\n",
    "def produce_X(sentences, lexicon):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        X_sample = [0 for _ in lexicon]\n",
    "        for word in word_tokenize(sentence):\n",
    "            root = lemmatizer.lemmatize(word.lower()).encode('utf-8')\n",
    "            if root in lexicon:\n",
    "                X_sample[lexicon.index(root)] += 1\n",
    "        \n",
    "        X.append(X_sample)\n",
    "    \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'however', b'these', b'approach', b'are', b'generally', b'basic']\n"
     ]
    }
   ],
   "source": [
    "sample_lexicon = create_lexicon(sentences)\n",
    "pprint(sample_lexicon[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = produce_X(sentences, sample_lexicon)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickles features generated for reuse\n",
    "\n",
    "with open('sample_X.npy', 'wb') as f:\n",
    "    np.save(f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories (index 0-5)\n",
    "0. LEGAL MEASURES\n",
    "1. TECHNICAL MEASURES\n",
    "2. ORGANIZATION MEASURES\n",
    "3. CAPACITY BUILDING\n",
    "4. COOPERATION\n",
    "5. CHILD ONLINE PROTECTION\n",
    "\n",
    "> Categories will be stored as a 1D array with each number corresponding to a category listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary stores label names and corresponding index to be turned on in the one hot vector.\n",
    "category_dict = {\n",
    "    u'LEGAL MEASURES' : 0,\n",
    "    u'TECHNICAL MEASURES' : 1,\n",
    "    u'ORGANIZATION MEASURES' : 2,\n",
    "    u'CAPACITY BUILDING' : 3,\n",
    "    u'COOPERATION' : 4,\n",
    "    u'CHILD ONLINE PROTECTION' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_y(tags):\n",
    "    return np.array([category_dict[tag[0][u'category'].upper()] for tag in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "y = produce_y(tags)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tags.npy', 'wb') as f:\n",
    "    np.save(f, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorflow Boilerplate\n",
    "To simplify the Tensorflow code, we will define a set of functions to delare variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(X, W, b, name='fc'):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.relu(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Now that the data has been processed it is now time to load the data and fit a model to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_X.npy\",\"rb\") as f:\n",
    "    X = np.load(f)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "with open(\"tags.npy\",\"rb\") as f:\n",
    "    y = np.load(f)\n",
    "\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_size = len(X[0])\n",
    "number_of_options = max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(vector):\n",
    "    def hot_or_not(i, j):\n",
    "        return 1 if i == j else 0\n",
    "    return [[hot_or_not(i, j) for j in number_of_options] for i in vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the Model\n",
    "Now we can create a neural network to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_placeholder = tf.placeholder(tf.float32, [None, lexicon_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = weight_variable([lexicon_size, 10])\n",
    "b1 = bias_variable([10])\n",
    "\n",
    "model = fc_layer(X_placeholder, w1, b1, name='fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2 = weight_variable([10, 10])\n",
    "b2 = bias_variable([10])\n",
    "\n",
    "model = fc_layer(X_placeholder, w2, b2, name='fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w3 = weight_variable([10, number_of_options])\n",
    "b3 = bias_variable([number_of_options])\n",
    "\n",
    "y_predicted = tf.matmul(model, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
