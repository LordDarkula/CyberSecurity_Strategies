{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-based Text Analytics of CyberSecurity Strategies\n",
    "Uses machine learning to calssify sentences from CyberSecurity documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These labels come from the headers in the cyberwellness profiles linked above**\n",
    "\n",
    "| Category               | Sub category |\n",
    "|------------------------| -------------|\n",
    "|LEGAL MEASURES          | CRIMINAL LEGISLATION, REGULATION AND COMPLIANCE|\n",
    "|TECHNICAL MEASURES      | CIRT, STANDARDS, CERTIFICATION|\n",
    "|ORGANIZATION MEASURES   | POLICY, ROADMAP FOR GOVERNANCE, RESPONSIBLE AGENCY, NATIONAL BENCHMARKING|\n",
    "|CAPACITY BUILDING       | STANDARDISATION DEVELOPMENT, MANPOWER DEVELOPMENT, PROFESSIONAL CERTIFICATION, AGENCY CERTIFICATION|\n",
    "|COOPERATION             | INTRA-STATE COOPERATION, INTRA-AGENCY COOPERATION, PUBLIC SECTOR PARTNERSHIP,  INTERNATIONAL COOPERATION|\n",
    "|CHILD ONLINE PROTECTION | NATIONAL LEGISLATION,  UN CONVENTION AND PROTOCOL, INSTITUTIONAL SUPPORT, REPORTING MECHANISM|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When using nltk for the first time, uncomment the following lines and run cell.\n",
    "# nltk.download must only be downloaded once\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-009e6538ccc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named token"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.token import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Country': u'Jordan',\n",
      "  u'sentence': u'However, these approaches: are generally basic; not systematic; subjective; have no clear definition or boundaries, are not thorough; do not meet international standards; and do not deal effectively with threats emerging from cyberspace.',\n",
      "  u'sentence_id,': u'ff30d97ab4',\n",
      "  u'tag': [{u'category': u'technical measures',\n",
      "            u'subcategory': [u'standards']}]},\n",
      " {u'Country': u'Jordan',\n",
      "  u'sentence': u'Strategies and policies developed by the private sector should augment, comply, and be consistent with this strategy.',\n",
      "  u'sentence_id,': u'e50e3676b6',\n",
      "  u'tag': [{u'category': u'organization measures',\n",
      "            u'subcategory': [u'policy']}]},\n",
      " {u'Country': u'Jordan',\n",
      "  u'sentence': u'security policy and role-based security responsibilities will have a higher rate of success in protecting critical information.',\n",
      "  u'sentence_id,': u'ddd832b614',\n",
      "  u'tag': [{u'category': u'organization measures',\n",
      "            u'subcategory': [u'policy']}]},\n",
      " {u'Country': u'Jordan',\n",
      "  u'sentence': u'Develop National Information Security Standards and Policies An authorized government entity, National Information Assurance and Security Agency (NIACSA) (Section 5), will develop and or customize nationwide cyberspace security overarching standards considering current The efforts of implementing nationwide overarching standards and policies will be assigned or managed by NIACSA (Section 5).',\n",
      "  u'sentence_id,': u'b8ea57d517',\n",
      "  u'tag': [{u'category': u'technical measures',\n",
      "            u'subcategory': [u'standards']},\n",
      "           {u'category': u'organization measures',\n",
      "            u'subcategory': [u'policy']}]}]\n"
     ]
    }
   ],
   "source": [
    "# Opens training data stored as Json and converts to Python list\n",
    "with open('results_concatenated.json') as f:    \n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples is 2045 \n",
      "\n",
      "First example is \n",
      "X: However, these approaches: are generally basic; not systematic; subjective; have no clear definition or boundaries, are not thorough; do not meet international standards; and do not deal effectively with threats emerging from cyberspace. \n",
      "\n",
      " y: [{u'category': u'technical measures', u'subcategory': [u'standards']}]\n"
     ]
    }
   ],
   "source": [
    "# Splits data into 3 parts, IDs, sentences, and tags\n",
    "\n",
    "# Optional\n",
    "sentence_ids = []\n",
    "\n",
    "# Lexicons created from sentences will be inputs\n",
    "sentences  = []\n",
    "\n",
    "# Tags will be outputs\n",
    "tags = []\n",
    "\n",
    "for input_val in data:\n",
    "    sentence_ids.append(input_val[u'sentence_id,'])\n",
    "    sentences.append(input_val[u'sentence'].encode(\"utf-8\"))\n",
    "    tags.append(input_val[u'tag'])\n",
    "\n",
    "print \"Number of training examples is {} \\n\".format(len(sentences))\n",
    "print \"First example is \\nX: {} \\n\\n y: {}\".format(sentences[0], tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates lexicon (list of unique words) from all training samples\n",
    "def create_lexicon(sentences):\n",
    "    lexicon = []\n",
    "    for sentence in sentences:\n",
    "            for sentence in sentences:\n",
    "                root = lemmatizer.lemmatize(word)\n",
    "                if root not in stop and root not in lexicon:\n",
    "                    lexicon.append(root)\n",
    "    \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates 2D array containing number of occurences of each word in lexicon in each sample\n",
    "def produce_X(sentences, lexicon):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        X_sample = [0 for _ in range(lexicon)]\n",
    "        for word in sentence:\n",
    "            if word.lower in lexicon:\n",
    "                X_sample[lexicon.index(word.lower)] += 1\n",
    "        \n",
    "        X.append(X_sample)\n",
    "    \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
